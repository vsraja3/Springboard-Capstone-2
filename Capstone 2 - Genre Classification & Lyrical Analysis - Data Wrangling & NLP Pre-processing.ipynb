{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import string\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data acquisition process complete after pulling our dataset containing the musical attributes of 10 thousand songs across 5 distinct genres through Spotify and subsequently pulling in the lyrics through Genius by searching each record and scraping the lyrics, we now have a dataframe that has just about all the information we will need to proceed with exploratory data analysis and predictive modeling. However, since we are working with text data with regards to the lyrics, we will first need to apply NLP (Natural Language Processing) techniques in order to clean our text data and present it in a format that will be more accessible to analyze visually and apply machine learning techniques to. We will use a few popular NLP pre-processing Python packages including NLTK, Spacy, Gensim, and RegEx along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in dataframe from Spotify Data Extraction section containing musical attributes\n",
    "spotify_df = pd.read_csv('spotify_genre_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, we will review the lyrical data of our first record in order to determine what potential pre-processing techniques need to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Produced by Rick Rubin]\\n\\n[Verse 1]\\nGettin\\' born in the state of Mississippi\\nPoppa was a copper and her momma was a hippie\\nIn Alabama, she would swing a hammer\\nPrice you gotta pay when you break the panorama\\nShe never knew that there was anything more than poor\\nWhat in the world, what does your company take me for?\\nBlack bandana, sweet Louisiana\\nRobbin\\' on a bank in the state of Indiana\\nShe\\'s a runner, rebel and a stunner\\nOn her merry way sayin\\', \"Baby, what you gonna—?\"\\nLookin\\' down the barrel of a hot metal .45\\nJust another way to survive\\n\\n[Chorus]\\nCalifornia, rest in peace\\nSimultaneous release\\nCalifornia, show your teeth\\nShe\\'s my priestess, I\\'m your priest, yeah, yeah\\n\\n[Verse 2]\\nShe\\'s a lover, baby and a fighter\\nShoulda seen her comin\\' when it got a little brighter\\nWith a name like Dani California\\nDay was gonna come when I was gonna mourn ya\\nA little loaded she was stealin\\' another breath\\nI love my baby to death\\n\\n[Chorus]\\nCalifornia, rest in peace\\nSimultaneous release\\nCalifornia, show your teeth\\nShe\\'s my priestess, I\\'m your priest, yeah, yeah\\n\\n[Bridge]\\nWho knew the other side of you?\\nWho knew what others died to prove?\\nToo true to say goodbye to you\\nToo true to say, say, say\\n\\n[Verse 3]\\nPush the fader, gifted animator\\nOne for the now and eleven for the later\\nNever made it up to Minnesota\\nNorth Dakota man was a gunnin\\' for the quota\\nDown in the Badlands she was savin\\' the best for last\\nIt only hurts when I laugh\\nGone too fast\\n\\n[Chorus]\\nCalifornia, rest in peace\\nSimultaneous release\\nCalifornia, show your teeth\\nShe\\'s my priestess, I\\'m your priest, yeah, yeah\\nCalifornia, rest in peace (Do svidaniya)\\nSimultaneous release (California)\\nCalifornia, show your teeth (Do svidaniya)\\nShe\\'s my priestess, I\\'m your priest, yeah, yeah\\n\\n[Guitar Solo]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "spotify_df['lyrics'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, there are a few easy fixes we will need to make to start off. The first of this is that all the lyrics include new line markets '\\n' in between each line of the lyrics as listed on the Genius website as a result of the HTML formatting that the Lyrics Genius wrapper scraped from. We will address this by applying the regular expression library function 're.sub'. This essentially acts as a find and replace that can be applied to any expression. In our case we will specify the string to find as '\\n' and replace it with a single space in order to keep words separated. Below this is applied to all rows in the dataframe using a for loop against the dataframe index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#There are new line markers in the plain text from the html scraping - remove these using a string replace with space\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['lyrics'][i] = re.sub(\"\\\\n\", \" \", spotify_df['lyrics'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Produced by Rick Rubin]  [Verse 1] Gettin\\' born in the state of Mississippi Poppa was a copper and her momma was a hippie In Alabama, she would swing a hammer Price you gotta pay when you break the panorama She never knew that there was anything more than poor What in the world, what does your company take me for? Black bandana, sweet Louisiana Robbin\\' on a bank in the state of Indiana She\\'s a runner, rebel and a stunner On her merry way sayin\\', \"Baby, what you gonna—?\" Lookin\\' down the barrel of a hot metal .45 Just another way to survive  [Chorus] California, rest in peace Simultaneous release California, show your teeth She\\'s my priestess, I\\'m your priest, yeah, yeah  [Verse 2] She\\'s a lover, baby and a fighter Shoulda seen her comin\\' when it got a little brighter With a name like Dani California Day was gonna come when I was gonna mourn ya A little loaded she was stealin\\' another breath I love my baby to death  [Chorus] California, rest in peace Simultaneous release California, show your teeth She\\'s my priestess, I\\'m your priest, yeah, yeah  [Bridge] Who knew the other side of you? Who knew what others died to prove? Too true to say goodbye to you Too true to say, say, say  [Verse 3] Push the fader, gifted animator One for the now and eleven for the later Never made it up to Minnesota North Dakota man was a gunnin\\' for the quota Down in the Badlands she was savin\\' the best for last It only hurts when I laugh Gone too fast  [Chorus] California, rest in peace Simultaneous release California, show your teeth She\\'s my priestess, I\\'m your priest, yeah, yeah California, rest in peace (Do svidaniya) Simultaneous release (California) California, show your teeth (Do svidaniya) She\\'s my priestess, I\\'m your priest, yeah, yeah  [Guitar Solo]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df['lyrics'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the function and reviewing the updated dataframe by checking the first row of lyrics, we can see that the new line markers have been successfully replaced. The next replacement we make is to remove the song section labels (ie. Verse, Chorus, Bridge, etc). We could choose to leave these markers in the lyrics, but considering they are not actually part of the raw text of the lyrics and could affect potential metrics such as word count and word frequency adversely, we will remove these by using a similar RegEx substitution. This would be tricky, but fortunately all of the section markers appear to be surrounded by square brackets (ie. '[Verse]') while there do not appear to be any square brackets around actual lyrics. As such, we can use the regular expression to remove all characters that are within square brackets and replace it with nothing, effectively deleting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "##There are song markers for sections of the lyrics (ie. verse, chorus, bridge, etc.) that can be removed\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['lyrics'][i] = re.sub('\\[.*?\\]', '',spotify_df['lyrics'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   Gettin\\' born in the state of Mississippi Poppa was a copper and her momma was a hippie In Alabama, she would swing a hammer Price you gotta pay when you break the panorama She never knew that there was anything more than poor What in the world, what does your company take me for? Black bandana, sweet Louisiana Robbin\\' on a bank in the state of Indiana She\\'s a runner, rebel and a stunner On her merry way sayin\\', \"Baby, what you gonna—?\" Lookin\\' down the barrel of a hot metal .45 Just another way to survive   California, rest in peace Simultaneous release California, show your teeth She\\'s my priestess, I\\'m your priest, yeah, yeah   She\\'s a lover, baby and a fighter Shoulda seen her comin\\' when it got a little brighter With a name like Dani California Day was gonna come when I was gonna mourn ya A little loaded she was stealin\\' another breath I love my baby to death   California, rest in peace Simultaneous release California, show your teeth She\\'s my priestess, I\\'m your priest, yeah, yeah   Who knew the other side of you? Who knew what others died to prove? Too true to say goodbye to you Too true to say, say, say   Push the fader, gifted animator One for the now and eleven for the later Never made it up to Minnesota North Dakota man was a gunnin\\' for the quota Down in the Badlands she was savin\\' the best for last It only hurts when I laugh Gone too fast   California, rest in peace Simultaneous release California, show your teeth She\\'s my priestess, I\\'m your priest, yeah, yeah California, rest in peace (Do svidaniya) Simultaneous release (California) California, show your teeth (Do svidaniya) She\\'s my priestess, I\\'m your priest, yeah, yeah  '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df['lyrics'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully removing the song section markers and verifying, the lyrical data is beginning to look just about clean as far as having complete words and no filler text. One additional step we will take is to remove all punctuation in order to preserve word meanings and prevent a single word from being counted twice (ex. 'what' and 'what?') as a result of a punctuation following it. We have a list of possible punctuations built into Python's string function methods which we can use to do this. Below you can see all the possible punctuation marks it contains. However, since apostrophes (') and hyphens (-) can potentially affect the meaning of certain words (ie. he'll, word-of-mouth), we will exclude these from the list of characters to remove and use the Pandas translate function to replace the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Check list of Python string stored punctuation\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "##Remove all punctuation as per list in string.punctuation EXCEPT hyphens and apostrophe's since they affect word meanings\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['lyrics'][i] = spotify_df['lyrics'][i].translate(str.maketrans('','','!\"#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed pre-processing to the point of removing punctuation and irrevelant text, we can begin to take steps to modify our remaining lyrical text to be better analyzed. While there is no standard as to what pre-processing steps need to be done for any NLP analysis, the most common ones include lowercasing, stemming, lemmatization, word tokenizing, removing stop words, and using counters and vectorizers. These techniques can be applied to any text analysis and be beneficial depending on the type of text. In the case of our current project on lyrical analysis, lowercase conversions, lemmatization, and word tokenizing are best applied. We begin by changing all words in the in each set of lyrics to lowercase using the Pandas lower method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "##Change all the words in the lyrics to lowercase in order to standardize and enable named entity recognition\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['lyrics'][i] = spotify_df['lyrics'][i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   gettin' born in the state of mississippi poppa was a copper and her momma was a hippie in alabama she would swing a hammer price you gotta pay when you break the panorama she never knew that there was anything more than poor what in the world what does your company take me for black bandana sweet louisiana robbin' on a bank in the state of indiana she's a runner rebel and a stunner on her merry way sayin' baby what you gonna— lookin' down the barrel of a hot metal 45 just another way to survive   california rest in peace simultaneous release california show your teeth she's my priestess i'm your priest yeah yeah   she's a lover baby and a fighter shoulda seen her comin' when it got a little brighter with a name like dani california day was gonna come when i was gonna mourn ya a little loaded she was stealin' another breath i love my baby to death   california rest in peace simultaneous release california show your teeth she's my priestess i'm your priest yeah yeah   who knew the other side of you who knew what others died to prove too true to say goodbye to you too true to say say say   push the fader gifted animator one for the now and eleven for the later never made it up to minnesota north dakota man was a gunnin' for the quota down in the badlands she was savin' the best for last it only hurts when i laugh gone too fast   california rest in peace simultaneous release california show your teeth she's my priestess i'm your priest yeah yeah california rest in peace do svidaniya simultaneous release california california show your teeth do svidaniya she's my priestess i'm your priest yeah yeah  \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df['lyrics'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The remaining pre-processing steps to take after converting all words to lowercase are all done at the word level and require the words of a document to be separated. In order to do this, we will apply the technique of word tokenizing, which takes a text document delimited by a character (in our case, by a single line space) and separates them into a list of word tokens. We will use the package NLTK which contains a number of different types of tokenizers which separate text into tokens by different metrics. We first attempt to use the basic 'word_tokenize' function and apply it to all rows to add the list of tokens to a new column in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "spotify_df['lyrics_tokenized'] = np.nan\n",
    "\n",
    "##Use NLTK word tokenize to examine lyrics column and create word tokens in the form of list in new column\n",
    "for i in spotify_df.index:\n",
    "    try:\n",
    "        spotify_df['lyrics_tokenized'][i] = word_tokenize(spotify_df['lyrics'][i])\n",
    "    except:\n",
    "        spotify_df['lyrics_tokenized'][i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [gettin, ', born, in, the, state, of, mississippi, poppa, was, a, copper, and, her, momma, was, a, hippie, in, alabama, she, would, swing, a, hammer, price, you, got, ta, pay, when, you, break, the, panorama, she, never, knew, that, there, was, anything, more, than, poor, what, in, the, world, what, does, your, company, take, me, for, black, bandana, sweet, louisiana, robbin, ', on, a, bank, in, the, state, of, indiana, she, 's, a, runner, rebel, and, a, stunner, on, her, merry, way, sayin, ', baby, what, you, gon, na, —, lookin, ', down, the, barrel, of, a, hot, metal, 45, ...]                                                    \n",
       "1    [psychic, spies, from, china, try, to, steal, your, mind, 's, elation, an, ', little, girls, from, sweden, dream, of, silver, screen, quotation, and, if, you, want, these, kind, of, dreams, it, 's, californication, it, 's, the, edge, of, the, world, and, all, of, western, civilization, the, sun, may, rise, in, the, east, at, least, it, 's, settled, in, a, final, location, it, 's, understood, that, hollywood, sells, californication, pay, your, surgeon, very, well, to, break, the, spell, of, aging, celebrity, skin, is, this, your, chin, or, is, that, war, you, 're, waging, first, born, unicorn, hardcore, soft, porn, dream, of, ...]\n",
       "2    [keep, you, in, the, dark, you, know, they, all, pretend, keep, you, in, the, dark, and, so, it, all, began, send, in, your, skeletons, sing, as, their, bones, go, marching, in, again, they, need, you, buried, deep, the, secrets, that, you, keep, are, at, the, ready, are, you, ready, i, 'm, finished, making, sense, done, pleading, ignorance, that, whole, defense, spinning, infinity, boy, the, wheel, is, spinning, me, it, 's, never, ending, never, ending, same, old, story, what, if, i, say, i, 'm, not, like, the, others, what, if, i, say, i, 'm, not, just, another, one, of, your, plays, ...]                                        \n",
       "3    [load, up, on, guns, bring, your, friends, it, 's, fun, to, lose, and, to, pretend, she, 's, over-bored, and, self-assured, oh, no, i, know, a, dirty, word, hello, hello, hello, hello, hello, hello, hello, how, low, hello, hello, hello, how, low, hello, hello, hello, with, the, lights, out, it, 's, less, dangerous, here, we, are, now, entertain, us, i, feel, stupid, and, contagious, here, we, are, now, entertain, us, a, mulatto, an, albino, a, mosquito, my, libido, yeah, hey, yay, i, 'm, worse, at, what, i, do, best, and, for, this, gift, i, feel, blessed, our, little, group, has, always, ...]                                     \n",
       "4    [scar, tissue, that, i, wish, you, saw, sarcastic, mister, know-it-all, close, your, eyes, and, i, 'll, kiss, you, 'cause, with, the, birds, i, 'll, share, with, the, birds, i, 'll, share, this, lonely, viewin, ', with, the, birds, i, 'll, share, this, lonely, viewin, ', push, me, up, against, the, wall, young, kentucky, girl, in, a, push-up, bra, ah, fallin, ', all, over, myself, to, lick, your, heart, and, taste, your, health, 'cause, with, the, birds, i, 'll, share, this, lonely, viewin, ', with, the, birds, i, 'll, share, this, lonely, viewin, ', with, the, birds, i, 'll, share, this, ...]                                     \n",
       "Name: lyrics_tokenized, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df['lyrics_tokenized'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon reviewing the first five rows of our newly tokenized columns, it looks like the tokenizer did a fairly adequate job at separating words correctly as we would expect. However, the one issue noticeable is that words that contain apostrophes, whether for contraction purposes (ie. \"we'll\" to \"we\" and \"'ll\") or shortening the pronunciation (ie. \"gettin'\" to \"gettin\" and \"'\"), are separated into tokens by the apostrophe. \n",
    "\n",
    "It is arguably subjective to determine whether these should indeed be tokenized into two words. In the case of the contraction particularly, the words are separate but combined together to shorten the length and ease of speech (ie. the word \"we'll\"  being \"we\" and \"will\"). However, in passing when read as  text or even heard in a song, these are phrased as a single word. As such, for the purposes of this project I determined it's best to keep these words together. In order to do this, I did some additional research into other tokenizers NLTK may enable and discovered that the Tweet Tokenizer, which as per it's named is meant to parse Twitter posts into tokens, does the job at tokenizing by separating words by keeping words connected by punctuation together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "##Use NLTK Tweet Tokenizer to attempt to tokenize contractions correctly, which were splitting using word tokenize\n",
    "for i in spotify_df.index:\n",
    "    try:\n",
    "        spotify_df['lyrics_tokenized'][i] = tknzr.tokenize(spotify_df['lyrics'][i])\n",
    "    except:\n",
    "        spotify_df['lyrics_tokenized'][i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [gettin, ', born, in, the, state, of, mississippi, poppa, was, a, copper, and, her, momma, was, a, hippie, in, alabama, she, would, swing, a, hammer, price, you, gotta, pay, when, you, break, the, panorama, she, never, knew, that, there, was, anything, more, than, poor, what, in, the, world, what, does, your, company, take, me, for, black, bandana, sweet, louisiana, robbin, ', on, a, bank, in, the, state, of, indiana, she's, a, runner, rebel, and, a, stunner, on, her, merry, way, sayin, ', baby, what, you, gonna, —, lookin, ', down, the, barrel, of, a, hot, metal, 45, just, another, way, ...]                                                                                  \n",
       "1    [psychic, spies, from, china, try, to, steal, your, mind's, elation, an, ', little, girls, from, sweden, dream, of, silver, screen, quotation, and, if, you, want, these, kind, of, dreams, it's, californication, it's, the, edge, of, the, world, and, all, of, western, civilization, the, sun, may, rise, in, the, east, at, least, it's, settled, in, a, final, location, it's, understood, that, hollywood, sells, californication, pay, your, surgeon, very, well, to, break, the, spell, of, aging, celebrity, skin, is, this, your, chin, or, is, that, war, you're, waging, first, born, unicorn, hardcore, soft, porn, dream, of, californication, dream, of, californication, dream, of, ...]\n",
       "2    [keep, you, in, the, dark, you, know, they, all, pretend, keep, you, in, the, dark, and, so, it, all, began, send, in, your, skeletons, sing, as, their, bones, go, marching, in, again, they, need, you, buried, deep, the, secrets, that, you, keep, are, at, the, ready, are, you, ready, i'm, finished, making, sense, done, pleading, ignorance, that, whole, defense, spinning, infinity, boy, the, wheel, is, spinning, me, it's, never, ending, never, ending, same, old, story, what, if, i, say, i'm, not, like, the, others, what, if, i, say, i'm, not, just, another, one, of, your, plays, you're, the, pretender, what, ...]                                                              \n",
       "3    [load, up, on, guns, bring, your, friends, it's, fun, to, lose, and, to, pretend, she's, over-bored, and, self-assured, oh, no, i, know, a, dirty, word, hello, hello, hello, hello, hello, hello, hello, how, low, hello, hello, hello, how, low, hello, hello, hello, with, the, lights, out, it's, less, dangerous, here, we, are, now, entertain, us, i, feel, stupid, and, contagious, here, we, are, now, entertain, us, a, mulatto, an, albino, a, mosquito, my, libido, yeah, hey, yay, i'm, worse, at, what, i, do, best, and, for, this, gift, i, feel, blessed, our, little, group, has, always, been, and, always, will, ...]                                                                \n",
       "4    [scar, tissue, that, i, wish, you, saw, sarcastic, mister, know-it-all, close, your, eyes, and, i'll, kiss, you, ', cause, with, the, birds, i'll, share, with, the, birds, i'll, share, this, lonely, viewin, ', with, the, birds, i'll, share, this, lonely, viewin, ', push, me, up, against, the, wall, young, kentucky, girl, in, a, push-up, bra, ah, fallin, ', all, over, myself, to, lick, your, heart, and, taste, your, health, ', cause, with, the, birds, i'll, share, this, lonely, viewin, ', with, the, birds, i'll, share, this, lonely, viewin, ', with, the, birds, i'll, share, this, lonely, view, blood, loss, in, ...]                                                            \n",
       "Name: lyrics_tokenized, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "spotify_df['lyrics_tokenized'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the Tweet Tokenizer in lieu of the the Word Tokenizer, we see that the contraction words have been correctly kept together as a single token. However, the words ending in an apostrophe are still being separated into two tokens. Since we would not want to count an apostrophe as a word, we can use the 'isalpha' method on each token in order to remove tokens such as the apostrophe that is not alpha-numeric.\n",
    "\n",
    "Beyond this, with our lyric text separated out into tokens we can continue to apply the other pre-processing steps that need to be done at the word level. The first of this was mentioned in removing non-alpha tokens. The second is to remove stop words, which are often defined as commonly used words that search engines that use NLP techniques are designed to ignore. These include pronouns such as \"I\", \"him\", \"her\" and articles such as \"The\" and \"and\". These words are likely to be common across all genres and thus not likely to be helpful in helping predict uniqueness across genres. As such, we will create a new column where these words are removed. \n",
    "\n",
    "Lastly, we have the techniques of stemming and lemmatization, which take the root of the words using different methods of application. These two techniques are generally mutually exclusive with the choice of using one or the other depending on the situation. For our purposes, we will use lemmatization which benefits from it's ability to retain the original meaning of the word, whereas stemming can often break down words to their root more accurately but sometimes lose the original meaning (ie. \"classification\" to \"class\"). \n",
    "\n",
    "Since all these techniques will change the word counts and overall lyrical data, we will make them in a new columns named as \"lyrics_cleaned\" so we can compare the before and after lyrics and apply in our upcoming analysis. Below I applied the three methods of removing non-alpha tokens, stopwords, and lemmatizing the remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "nltk.download('wordnet', 'stopwords')\n",
    "spotify_df['lyrics_cleaned'] = np.NaN\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "##Change all the words in the lyrics to lowercase in order to standardize and enable named entity recognition\n",
    "for i in spotify_df[spotify_df['lyrics_cleaned'].isnull()].head(1000).index:\n",
    "    spotify_df['lyrics_cleaned'][i] = [i for i in spotify_df['lyrics_tokenized'][i] if i.isalpha()]\n",
    "    spotify_df['lyrics_cleaned'][i] = [i for i in spotify_df['lyrics_cleaned'][i] if not i in stopwords.words('english')]\n",
    "    spotify_df['lyrics_cleaned'][i] = [lemmatizer.lemmatize(i) for i in spotify_df['lyrics_cleaned'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [gettin, born, state, mississippi, poppa, copper, momma, hippie, alabama, would, swing, hammer, price, gotta, pay, break, panorama, never, knew, anything, poor, world, company, take, black, bandana, sweet, louisiana, robbin, bank, state, indiana, runner, rebel, stunner, merry, way, sayin, baby, gonna, lookin, barrel, hot, metal, another, way, survive, california, rest, peace, simultaneous, release, california, show, teeth, priestess, priest, yeah, yeah, lover, baby, fighter, shoulda, seen, comin, got, little, brighter, name, like, dani, california, day, gonna, come, gonna, mourn, ya, little, loaded, stealin, another, breath, love, baby, death, california, rest, peace, simultaneous, release, california, show, teeth, priestess, priest, yeah, yeah, knew, side, ...]                                                                              \n",
       "1    [psychic, spies, china, try, steal, elation, little, girls, sweden, dream, silver, screen, quotation, want, kind, dreams, californication, edge, world, western, civilization, sun, may, rise, east, least, settled, final, location, understood, hollywood, sells, californication, pay, surgeon, well, break, spell, aging, celebrity, skin, chin, war, waging, first, born, unicorn, hardcore, soft, porn, dream, californication, dream, californication, dream, californication, dream, californication, marry, girl, fairy, world, constellation, teenage, bride, baby, inside, getting, high, information, buy, star, boulevard, californication, space, may, final, frontier, made, hollywood, basement, cobain, hear, spheres, singing, songs, station, station, far, away, californication, born, raised, praise, control, population, well, mean, vacation, first, ...]\n",
       "Name: lyrics_cleaned, dtype: object"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', -1)\n",
    "spotify_df['lyrics_cleaned'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these final techniques applied, we finally have a dataset which contains a raw text form of the lyrics, a cleaned text form, a tokenized version including all tokens, as well as a cleaned token form. We will now proceed to add a couple additional fields which can help assist in our EDA and ML portion. Given that these words are tokenized and text data, we cannot compare these directly with our other numerical data. However, we can derive numerical data from our text in the form of word counts, which we proceed to do below by taking the length of the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spotify_df['word_count'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "##Find the total word count of each song by using the len function on the list for each row\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['word_count'][i] = len(spotify_df['lyrics_tokenized'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>track_id</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>full_key</th>\n",
       "      <th>key</th>\n",
       "      <th>mode</th>\n",
       "      <th>...</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_minutes</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_tokenized</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dani California</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Stadium Arcadium</td>\n",
       "      <td>10Nmj3JCNoMeBQ87uw5j8k</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.913</td>\n",
       "      <td>C Major</td>\n",
       "      <td>C</td>\n",
       "      <td>Major</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.730</td>\n",
       "      <td>96.184</td>\n",
       "      <td>4.702667</td>\n",
       "      <td>282160</td>\n",
       "      <td>4</td>\n",
       "      <td>gettin' born in the state of mississippi po...</td>\n",
       "      <td>[gettin, ', born, in, the, state, of, mississi...</td>\n",
       "      <td>306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Californication</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Californication (Deluxe Edition)</td>\n",
       "      <td>48UPSzbZjgc449aqz8bxox</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.767</td>\n",
       "      <td>A Minor</td>\n",
       "      <td>A</td>\n",
       "      <td>Minor</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.328</td>\n",
       "      <td>96.483</td>\n",
       "      <td>5.495550</td>\n",
       "      <td>329733</td>\n",
       "      <td>4</td>\n",
       "      <td>psychic spies from china try to steal your mi...</td>\n",
       "      <td>[psychic, spies, from, china, try, to, steal, ...</td>\n",
       "      <td>286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Pretender</td>\n",
       "      <td>Foo Fighters</td>\n",
       "      <td>Echoes, Silence, Patience &amp; Grace</td>\n",
       "      <td>7x8dCjCr0x6x2lXKujYD34</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.959</td>\n",
       "      <td>A Major</td>\n",
       "      <td>A</td>\n",
       "      <td>Major</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.365</td>\n",
       "      <td>172.984</td>\n",
       "      <td>4.489550</td>\n",
       "      <td>269373</td>\n",
       "      <td>4</td>\n",
       "      <td>keep you in the dark you know they all preten...</td>\n",
       "      <td>[keep, you, in, the, dark, you, know, they, al...</td>\n",
       "      <td>461.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smells Like Teen Spirit</td>\n",
       "      <td>Nirvana</td>\n",
       "      <td>Nevermind (Remastered)</td>\n",
       "      <td>5ghIJDpPoe3CfHMGu71E6T</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.912</td>\n",
       "      <td>C# Major</td>\n",
       "      <td>C#</td>\n",
       "      <td>Major</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.720</td>\n",
       "      <td>116.761</td>\n",
       "      <td>5.032000</td>\n",
       "      <td>301920</td>\n",
       "      <td>4</td>\n",
       "      <td>load up on guns bring your friends it's fun t...</td>\n",
       "      <td>[load, up, on, guns, bring, your, friends, it'...</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scar Tissue</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Californication (Deluxe Edition)</td>\n",
       "      <td>1G391cbiT3v3Cywg8T7DM1</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.717</td>\n",
       "      <td>C Major</td>\n",
       "      <td>C</td>\n",
       "      <td>Major</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.547</td>\n",
       "      <td>88.969</td>\n",
       "      <td>3.598450</td>\n",
       "      <td>215907</td>\n",
       "      <td>4</td>\n",
       "      <td>scar tissue that i wish you saw sarcastic mis...</td>\n",
       "      <td>[scar, tissue, that, i, wish, you, saw, sarcas...</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                track_name                 artist  \\\n",
       "0          Dani California  Red Hot Chili Peppers   \n",
       "1          Californication  Red Hot Chili Peppers   \n",
       "2            The Pretender           Foo Fighters   \n",
       "3  Smells Like Teen Spirit                Nirvana   \n",
       "4              Scar Tissue  Red Hot Chili Peppers   \n",
       "\n",
       "                               album                track_id playlist_name  \\\n",
       "0                   Stadium Arcadium  10Nmj3JCNoMeBQ87uw5j8k          Rock   \n",
       "1   Californication (Deluxe Edition)  48UPSzbZjgc449aqz8bxox          Rock   \n",
       "2  Echoes, Silence, Patience & Grace  7x8dCjCr0x6x2lXKujYD34          Rock   \n",
       "3             Nevermind (Remastered)  5ghIJDpPoe3CfHMGu71E6T          Rock   \n",
       "4   Californication (Deluxe Edition)  1G391cbiT3v3Cywg8T7DM1          Rock   \n",
       "\n",
       "   danceability  energy  full_key key   mode  ...  instrumentalness  liveness  \\\n",
       "0         0.556   0.913   C Major   C  Major  ...          0.000009     0.346   \n",
       "1         0.592   0.767   A Minor   A  Minor  ...          0.001650     0.127   \n",
       "2         0.433   0.959   A Major   A  Major  ...          0.000000     0.028   \n",
       "3         0.502   0.912  C# Major  C#  Major  ...          0.000173     0.106   \n",
       "4         0.595   0.717   C Major   C  Major  ...          0.002740     0.108   \n",
       "\n",
       "   valence    tempo  duration_minutes  duration_ms  time_signature  \\\n",
       "0    0.730   96.184          4.702667       282160               4   \n",
       "1    0.328   96.483          5.495550       329733               4   \n",
       "2    0.365  172.984          4.489550       269373               4   \n",
       "3    0.720  116.761          5.032000       301920               4   \n",
       "4    0.547   88.969          3.598450       215907               4   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0     gettin' born in the state of mississippi po...   \n",
       "1   psychic spies from china try to steal your mi...   \n",
       "2   keep you in the dark you know they all preten...   \n",
       "3   load up on guns bring your friends it's fun t...   \n",
       "4   scar tissue that i wish you saw sarcastic mis...   \n",
       "\n",
       "                                    lyrics_tokenized word_count  \n",
       "0  [gettin, ', born, in, the, state, of, mississi...      306.0  \n",
       "1  [psychic, spies, from, china, try, to, steal, ...      286.0  \n",
       "2  [keep, you, in, the, dark, you, know, they, al...      461.0  \n",
       "3  [load, up, on, guns, bring, your, friends, it'...      249.0  \n",
       "4  [scar, tissue, that, i, wish, you, saw, sarcas...      248.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.reset_option('display.max_colwidth')\n",
    "spotify_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the word counts, we did some quick visual analysis to see what the distribution of word counts is across our dataset. Below you can see a box plot of the word count which visualized the middle 50 percent of the data along with the percentil distribution and any outliers. Somewhat surpringly, there are a considerable amount of serious outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAADuCAYAAAAN3LFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVeGCY2ZmVeGCY2ZmVfH/AS5PWh/QbzB5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a5b6822e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(y='word_count', data=spotify_df.sort_values(by=\"word_count\", ascending=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Word Count: 0.0\n",
      "Median Word Count: 299.0\n",
      "Maximum Word Count: 155275.0\n"
     ]
    }
   ],
   "source": [
    "min_word_count = np.min(spotify_df['word_count'])\n",
    "med_word_count = np.median(spotify_df['word_count'])\n",
    "max_word_count = np.max(spotify_df['word_count'])\n",
    "\n",
    "print(\"Minimum Word Count:\", min_word_count)\n",
    "print(\"Median Word Count:\", med_word_count)\n",
    "print(\"Maximum Word Count:\", max_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 : 0.0\n",
      "0.1 : 76.0\n",
      "0.15000000000000002 : 143.0\n",
      "0.2 : 179.0\n",
      "0.25 : 203.0\n",
      "0.3 : 225.0\n",
      "0.35000000000000003 : 245.0\n",
      "0.4 : 263.0\n",
      "0.45 : 281.0\n",
      "0.5 : 299.0\n",
      "0.55 : 318.0\n",
      "0.6000000000000001 : 340.0\n",
      "0.6500000000000001 : 363.0\n",
      "0.7000000000000001 : 392.0\n",
      "0.7500000000000001 : 429.0\n",
      "0.8 : 484.0\n",
      "0.8500000000000001 : 555.0\n",
      "0.9000000000000001 : 663.0\n",
      "0.9500000000000001 : 825.0\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(.05, 1.0, .05):\n",
    "    print(i, \":\", spotify_df['word_count'].quantile(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running some basic summary statistics on our word counts shows a median word count of 299 and a maximum of 155275, clearly a massive discrepancy. I sought to break this down further by determing what percentage of the data is above a certain word count, and did so by breaking down the quantiles by every 5 percent. As shown, 95% of the data contains 825 word tokens or less. As such, I examined a few of the rows with words above that and discovered that the lyrics were pulled incorrectly from the Genius API call, resulting in massive amounts of texts from novels and track listings and other text unrelated to the actual songs. As such, I proceeded to simply clear the lyrics of the top 5 percent of the data since these largely represented outliers and would not be accurate to use for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Clear lyrics on the upper 5 percent since they are likely innacurate\n",
    "spotify_df.loc[spotify_df['word_count'] > 825, 'lyrics'] = ''\n",
    "spotify_df.loc[spotify_df['word_count'] > 825, 'lyrics_tokenized'] = pd.Series([])\n",
    "spotify_df.loc[spotify_df['word_count'] > 825, 'word_count'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAADuCAYAAADMW/vrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nucF8JcoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3c4a7828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(y='word_count', data=spotify_df.sort_values(by=\"word_count\", ascending=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clearing the data and visualizing the word counts again via a box plot, we not see a much more reasonable distribution of word counts with the bulk hovering around 200 to 400 words. There still do appear to be outliers in the 700-800 range, but upon spot checking a few of these discovered they are mostly accurate lyrics and true outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunraja/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in spotify_df[spotify_df['lyrics_tokenized'].isnull()].index:\n",
    "    spotify_df['lyrics_tokenized'][i] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the word count of the original token list generated, I thought it would be useful to also generate a word count of the cleaned dataframe. In addition, another potential measurable metric is to count the unique words in a set of lyrics. As such I computed the quantity of cleaned lyrics tokens as well as unique tokens for both the raw and cleaned versions and updated the dataframe with a column for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "##Find the total word count of each song by using the len function on the list for each row\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['word_count'][i] = len(spotify_df['lyrics_tokenized'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "spotify_df['word_count_unique'] = np.nan\n",
    "\n",
    "##Find the total word count of each song by using the len function on the list for each row\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['lyrics_unique'][i] = len(set(spotify_df['lyrics_tokenized'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "spotify_df['word_count_cleaned'] = np.nan\n",
    "\n",
    "##Find the total word count of each song by using the len function on the list for each row\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['word_count_cleaned'][i] = len(set(spotify_df['lyrics_cleaned'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "spotify_df['word_count_cleaned_unique'] = np.nan\n",
    "\n",
    "##Find the total word count of each song by using the len function on the list for each row\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['word_count_cleaned_unique'][i] = len(set(spotify_df['lyrics_cleaned'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our datafram just about prepared, in order gain additional insights into potential topics and potentially important words across the sets of all lyrics, we used Spacy's named entity recognition function to add a column including named entity as well as the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "spotify_df['entity_count_unique'] = np.nan\n",
    "\n",
    "##Find the total word count of each song by using the len function on the list for each row\n",
    "for i in spotify_df.index:\n",
    "    spotify_df['entity_count_unique'][i] = len(set(spotify_df['named_entities'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunraja/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in spotify_df[spotify_df['named_entities'].isnull()].head(1000).index:\n",
    "    spotify_df['named_entities'][i] = ner(str(spotify_df.iloc[i]['lyrics'])).ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>track_id</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>full_key</th>\n",
       "      <th>key</th>\n",
       "      <th>mode</th>\n",
       "      <th>...</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_minutes</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_tokenized</th>\n",
       "      <th>word_count</th>\n",
       "      <th>lyrics_cleaned</th>\n",
       "      <th>named_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dani California</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Stadium Arcadium</td>\n",
       "      <td>10Nmj3JCNoMeBQ87uw5j8k</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.913</td>\n",
       "      <td>C Major</td>\n",
       "      <td>C</td>\n",
       "      <td>Major</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730</td>\n",
       "      <td>96.184</td>\n",
       "      <td>4.702667</td>\n",
       "      <td>282160</td>\n",
       "      <td>4</td>\n",
       "      <td>gettin' born in the state of mississippi po...</td>\n",
       "      <td>[gettin, ', born, in, the, state, of, mississi...</td>\n",
       "      <td>306.0</td>\n",
       "      <td>[gettin, born, state, mississippi, poppa, copp...</td>\n",
       "      <td>[(mississippi, poppa), (alabama), (sweet, loui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Californication</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Californication (Deluxe Edition)</td>\n",
       "      <td>48UPSzbZjgc449aqz8bxox</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.767</td>\n",
       "      <td>A Minor</td>\n",
       "      <td>A</td>\n",
       "      <td>Minor</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328</td>\n",
       "      <td>96.483</td>\n",
       "      <td>5.495550</td>\n",
       "      <td>329733</td>\n",
       "      <td>4</td>\n",
       "      <td>psychic spies from china try to steal your mi...</td>\n",
       "      <td>[psychic, spies, from, china, try, to, steal, ...</td>\n",
       "      <td>286.0</td>\n",
       "      <td>[psychic, spy, china, try, steal, elation, lit...</td>\n",
       "      <td>((china), (sweden), (east), (hollywood), (holl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Pretender</td>\n",
       "      <td>Foo Fighters</td>\n",
       "      <td>Echoes, Silence, Patience &amp; Grace</td>\n",
       "      <td>7x8dCjCr0x6x2lXKujYD34</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.959</td>\n",
       "      <td>A Major</td>\n",
       "      <td>A</td>\n",
       "      <td>Major</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365</td>\n",
       "      <td>172.984</td>\n",
       "      <td>4.489550</td>\n",
       "      <td>269373</td>\n",
       "      <td>4</td>\n",
       "      <td>keep you in the dark you know they all preten...</td>\n",
       "      <td>[keep, you, in, the, dark, you, know, they, al...</td>\n",
       "      <td>461.0</td>\n",
       "      <td>[keep, dark, know, pretend, keep, dark, began,...</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smells Like Teen Spirit</td>\n",
       "      <td>Nirvana</td>\n",
       "      <td>Nevermind (Remastered)</td>\n",
       "      <td>5ghIJDpPoe3CfHMGu71E6T</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.912</td>\n",
       "      <td>C# Major</td>\n",
       "      <td>C#</td>\n",
       "      <td>Major</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720</td>\n",
       "      <td>116.761</td>\n",
       "      <td>5.032000</td>\n",
       "      <td>301920</td>\n",
       "      <td>4</td>\n",
       "      <td>load up on guns bring your friends it's fun t...</td>\n",
       "      <td>[load, up, on, guns, bring, your, friends, it'...</td>\n",
       "      <td>249.0</td>\n",
       "      <td>[load, gun, bring, friend, fun, lose, pretend,...</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scar Tissue</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Californication (Deluxe Edition)</td>\n",
       "      <td>1G391cbiT3v3Cywg8T7DM1</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.717</td>\n",
       "      <td>C Major</td>\n",
       "      <td>C</td>\n",
       "      <td>Major</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547</td>\n",
       "      <td>88.969</td>\n",
       "      <td>3.598450</td>\n",
       "      <td>215907</td>\n",
       "      <td>4</td>\n",
       "      <td>scar tissue that i wish you saw sarcastic mis...</td>\n",
       "      <td>[scar, tissue, that, i, wish, you, saw, sarcas...</td>\n",
       "      <td>248.0</td>\n",
       "      <td>[scar, tissue, wish, saw, sarcastic, mister, c...</td>\n",
       "      <td>((kentucky), (fallin), (ma), (pa), (viewin), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                track_name                 artist  \\\n",
       "0          Dani California  Red Hot Chili Peppers   \n",
       "1          Californication  Red Hot Chili Peppers   \n",
       "2            The Pretender           Foo Fighters   \n",
       "3  Smells Like Teen Spirit                Nirvana   \n",
       "4              Scar Tissue  Red Hot Chili Peppers   \n",
       "\n",
       "                               album                track_id playlist_name  \\\n",
       "0                   Stadium Arcadium  10Nmj3JCNoMeBQ87uw5j8k          Rock   \n",
       "1   Californication (Deluxe Edition)  48UPSzbZjgc449aqz8bxox          Rock   \n",
       "2  Echoes, Silence, Patience & Grace  7x8dCjCr0x6x2lXKujYD34          Rock   \n",
       "3             Nevermind (Remastered)  5ghIJDpPoe3CfHMGu71E6T          Rock   \n",
       "4   Californication (Deluxe Edition)  1G391cbiT3v3Cywg8T7DM1          Rock   \n",
       "\n",
       "   danceability  energy  full_key key   mode  ...  valence    tempo  \\\n",
       "0         0.556   0.913   C Major   C  Major  ...    0.730   96.184   \n",
       "1         0.592   0.767   A Minor   A  Minor  ...    0.328   96.483   \n",
       "2         0.433   0.959   A Major   A  Major  ...    0.365  172.984   \n",
       "3         0.502   0.912  C# Major  C#  Major  ...    0.720  116.761   \n",
       "4         0.595   0.717   C Major   C  Major  ...    0.547   88.969   \n",
       "\n",
       "   duration_minutes  duration_ms  time_signature  \\\n",
       "0          4.702667       282160               4   \n",
       "1          5.495550       329733               4   \n",
       "2          4.489550       269373               4   \n",
       "3          5.032000       301920               4   \n",
       "4          3.598450       215907               4   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0     gettin' born in the state of mississippi po...   \n",
       "1   psychic spies from china try to steal your mi...   \n",
       "2   keep you in the dark you know they all preten...   \n",
       "3   load up on guns bring your friends it's fun t...   \n",
       "4   scar tissue that i wish you saw sarcastic mis...   \n",
       "\n",
       "                                    lyrics_tokenized  word_count  \\\n",
       "0  [gettin, ', born, in, the, state, of, mississi...       306.0   \n",
       "1  [psychic, spies, from, china, try, to, steal, ...       286.0   \n",
       "2  [keep, you, in, the, dark, you, know, they, al...       461.0   \n",
       "3  [load, up, on, guns, bring, your, friends, it'...       249.0   \n",
       "4  [scar, tissue, that, i, wish, you, saw, sarcas...       248.0   \n",
       "\n",
       "                                      lyrics_cleaned  \\\n",
       "0  [gettin, born, state, mississippi, poppa, copp...   \n",
       "1  [psychic, spy, china, try, steal, elation, lit...   \n",
       "2  [keep, dark, know, pretend, keep, dark, began,...   \n",
       "3  [load, gun, bring, friend, fun, lose, pretend,...   \n",
       "4  [scar, tissue, wish, saw, sarcastic, mister, c...   \n",
       "\n",
       "                                      named_entities  \n",
       "0  [(mississippi, poppa), (alabama), (sweet, loui...  \n",
       "1  ((china), (sweden), (east), (hollywood), (holl...  \n",
       "2                                                 ()  \n",
       "3                                                 ()  \n",
       "4  ((kentucky), (fallin), (ma), (pa), (viewin), (...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.reset_option('max_colwidth')\n",
    "spotify_df[~spotify_df['named_entities'].isnull()].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Load named entities through Spacy and loop through lyrics to create new columns containing list of entities for each song\n",
    "ner = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "spotify_df['named_entities'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, in order to see the breakdown of unique tracks, artists, and albums in each genre for potential EDA, I added columns for tracking the count of each of these across genres. Finally we have a dataframe with all the preprocessing complete and necessary metrics built into our dataframe. We will save off the updated dataframe into a CSV and proceed to use it in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "genres = ['Rock', 'Country', 'Hip-Hop', 'Pop', 'Electronic/Dance']\n",
    "spotify_df['count_artist'] = np.nan\n",
    "spotify_df['count_track'] = np.nan\n",
    "spotify_df['count_album'] = np.nan\n",
    "\n",
    "for genre in genres:\n",
    "    for i in spotify_df[spotify_df['playlist_name'] == genre].index:  \n",
    "        spotify_df['count_artist'][i] = spotify_df[spotify_df['playlist_name'] == genre]['artist'].nunique()\n",
    "        spotify_df['count_track'][i] = spotify_df[spotify_df['playlist_name'] == genre]['track_name'].nunique()\n",
    "        spotify_df['count_album'][i] = spotify_df[spotify_df['playlist_name'] == genre]['album'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read dataframe into CSV file to use in the next portion\n",
    "spotify_df.to_csv(\"spotify_genre_df_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
